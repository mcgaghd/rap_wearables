{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef0037b",
   "metadata": {},
   "source": [
    "# Extracting data using the UKB RAP\n",
    "\n",
    "## Introduction\n",
    "In this notebook, we will dispense data for analysis using the RAP. \n",
    "\n",
    "## About this notebook\n",
    "\n",
    "This is a Python notebook.\n",
    "\n",
    "Data on the RAP is dispensed using Spark (a data processing framework well-suited to big data which works by distributing data and data processing over several machines). We will access the data using [pyspark](https://pypi.org/project/pyspark/), a Python interface to Spark.\n",
    "\n",
    "Here, we will also do some manipulation of the data using pyspark. There are alternatives: you could convert earlier to a [pandas](https://pandas.pydata.org/) or [koalas](https://koalas.readthedocs.io/en/latest/) data frame and manipulate those. Be aware that working in pandas no longer takes advantage of the distributed aspect of Spark, so you may run into issues with the size of the data. koalas looks and feels like pandas but makes use of the distributed aspect of Spark.\n",
    "\n",
    "## How to run this notebook\n",
    "\n",
    "This notebook should be run in a *Spark in JupyterLab* session. Read how to set it up [here](https://dnanexus.gitbook.io/uk-biobank-rap/working-on-the-research-analysis-platform/using-spark-to-analyze-tabular-data).\n",
    "\n",
    "## Set up the session\n",
    "\n",
    "We load modules we'll use, and set up the Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b18d431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import dxpy # tools starting with 'dx' are from the DNANexus ecosystem\n",
    "import dxdata\n",
    "from pyspark.sql.functions import when, concat_ws\n",
    "from re import sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68b2cd26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/cluster/dnax/jars/dnanexus-api-0.1.0-SNAPSHOT-jar-with-dependencies.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/cluster/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-05 14:09:03.567 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-09-05 14:09:04.640 WARN  Utils:69 - Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 43000. Attempting port 43001.\n",
      "2023-09-05 14:09:04.942 WARN  MetricsReporter:84 - No metrics configured for reporting\n",
      "2023-09-05 14:09:04.944 WARN  LineProtoUsageReporter:48 - Telegraf configurations: url [metrics.push.telegraf.hostport], user [metrics.push.telegraf.user] or password [metrics.push.telegraf.password] missing.\n",
      "2023-09-05 14:09:04.944 WARN  MetricsReporter:117 - metrics.scraping.httpserver.port\n"
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext()\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ed3f06",
   "metadata": {},
   "source": [
    "## Dispense a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb5d32dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dispensed_database = dxpy.find_one_data_object(\n",
    "    classname=\"database\", \n",
    "    name=\"app*\", \n",
    "    folder=\"/\", \n",
    "    name_mode=\"glob\", \n",
    "    describe=True)\n",
    "dispensed_database_name = dispensed_database[\"describe\"][\"name\"]\n",
    "\n",
    "dispensed_dataset = dxpy.find_one_data_object(\n",
    "    typename=\"Dataset\", \n",
    "    name=\"app*.dataset\", \n",
    "    folder=\"/\", \n",
    "    name_mode=\"glob\")\n",
    "dispensed_dataset_id = dispensed_dataset[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccebc94",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efd79eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dxdata.load_dataset(id=dispensed_dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1b9164",
   "metadata": {},
   "source": [
    "## Tabular participant data\n",
    "\n",
    "We first load the \"straightforward\" tabular participant data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0367d15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "participant = dataset[\"participant\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30387f7",
   "metadata": {},
   "source": [
    "Next, we'll load a list of fields we might want for our analysis. We use this utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a0337fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_column_list(file_name):\n",
    "    \"\"\"Load list of UK Biobank column IDs from file\n",
    "    Column IDs can be obtained from the UK Biobank showcase: https://biobank.ndph.ox.ac.uk/showcase/index.cgi\n",
    "    e.g. 90012 refers to the recommended variable for accelerometer measured physical activity\n",
    "        https://biobank.ndph.ox.ac.uk/showcase/field.cgi?id=90012\n",
    "        \n",
    "    This function is due to Aiden Doherty.\n",
    "\n",
    "    :param str file_name: Name of file listing UK Biobank column IDs\n",
    "    :return: list of IDs\n",
    "    :rtype: list\n",
    "    :Example:\n",
    "    >>> load_column_list(\"field_list.txt\")\n",
    "    [\"31\", \"34\", \"52\" ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    column_IDs = []\n",
    "    \n",
    "    with open(file_name) as f:\n",
    "            for line in f:\n",
    "                li = line.strip()\n",
    "                if \"#\" in li:\n",
    "                    li = li.split(\"#\")[0].strip()\n",
    "                if not li.startswith(\"#\") and not li==\"\":\n",
    "                    column_IDs += [li]\n",
    "    return column_IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0773981",
   "metadata": {},
   "source": [
    "We read in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dc30e34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_list = load_column_list(\"candidate_field_list.txt\") # This is a text file listing the field IDs we'll use\n",
    "# You should change this file location as appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2031514c",
   "metadata": {},
   "source": [
    "Next, we'll use these field numbers to work out the [field *names*](https://dnanexus.gitbook.io/uk-biobank-rap/frequently-asked-questions#how-are-column-names-determined-for-the-dispensed-database) we need.\n",
    "\n",
    "In particular, UK Biobank fields sometimes have *instance* and *array* indices, which are [indicated in the field names](https://dnanexus.gitbook.io/uk-biobank-rap/frequently-asked-questions#how-are-column-names-determined-for-the-dispensed-database). Instances refer to different occasions when participants were measured (e.g. different visits to the assessment centre). For most instanced variables, [instance 0 is baseline, instance 1 is a resurvey among a limited number of participants, instance 2 is the imaging visit, and instance 3 is the first repeat imaging visit](https://biobank.ndph.ox.ac.uk/ukb/instance.cgi?id=2). At each instance after baseline, only a subset of participants were measured. Array indices are used where variables have multiple items e.g. participants gave multiple answers (e.g. employment history) or had multiple measurements on a single occasion (e.g. blood pressure).\n",
    "\n",
    "We write utility functions to get names for a particular field and instance. These functions are based on [functions from DNANexus available on GitHub here](https://github.com/dnanexus/OpenBio/blob/master/UKB_notebooks/ukb-rap-pheno-basic.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76739e98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fields_for_id(field_id):\n",
    "    from distutils.version import LooseVersion\n",
    "    field_id = str(field_id)\n",
    "    fields = participant.find_fields(name_regex=r'^p{}(_i\\d+)?(_a\\d+)?$'.format(field_id))\n",
    "    return sorted(fields, key=lambda f: LooseVersion(f.name))\n",
    "\n",
    "def field_names_for_id_instanced(field_id, instance=\"i0\", include_non_instanced=True):\n",
    "    candidate_fields = [f.name for f in fields_for_id(field_id)]\n",
    "    return_fields = [f for f in candidate_fields if instance in f]\n",
    "    if (include_non_instanced): # This means fields without instancing (e.g. sex) will be included, and defaults to true\n",
    "        return_fields += [f for f in candidate_fields if \"_i\" not in f]\n",
    "    return return_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d08433",
   "metadata": {},
   "source": [
    "In this analysis, we will take non-accelerometer participant data from baseline. This means we need variables from instance 0 or with no instancing (some variables, like sex, have one value across all visits; the accelerometer variables are also not instanced):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "138b19ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_133/3480082101.py:5: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  return sorted(fields, key=lambda f: LooseVersion(f.name))\n"
     ]
    }
   ],
   "source": [
    "field_list = [\"eid\"]\n",
    "for col in column_list:\n",
    "    field_list += field_names_for_id_instanced(col, instance=\"i0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17507c0",
   "metadata": {},
   "source": [
    "An aside: as accelerometer wear occurred several years after baseline assessment, and some participants have undergone repeat assessments between baseline assessment and accelerometer wear, so we might alternatively use data from the most recent assessment before accelerometer wear for covariate adjustment. Given only a small proportion of participants have a second visit before accelerometer wear, for simplicity we ignore that here. However, we will add a couple of fields on self-reported conditions from repeat assessments, so we can more rigorously account for prevalent disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f698157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "field_list += [\"p6150_i1\", \"p6150_i2\", \"p53_i1\", \"p53_i2\"] # we'll take a couple of extra fields to allow us to\n",
    "# more rigorously exclude people with prevalent disease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f9d3f",
   "metadata": {},
   "source": [
    "If we just input the field list like this, we'll retrieve fields with their UK Biobank field names (format: [pX_iX_aX](https://dnanexus.gitbook.io/uk-biobank-rap/working-on-the-research-analysis-platform/using-spark-to-analyze-tabular-data#database-columns)). We can set up some aliases with more human readable names:\n",
    "\n",
    "[We use two different approaches here to illustrate them- you can use either, neither (i.e. use the fields with their original field names), or a mix.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1957fad6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We'll handcraft some of the names to slot neatly into later code\n",
    "field_list_aliases = {\n",
    "    \"eid\" : \"eid\", \n",
    "    \"p31\" :  \"sex\",\n",
    "    \"p52\" : \"month_birth\", \n",
    "    \"p34\" : \"year_birth\", \n",
    "    \"p54_i0\" :\"ukb_assess_cent\", \n",
    "    \"p21000_i0\" : \"ethnicity_raw\",\n",
    "    \"p189\" : \"tdi_raw\",\n",
    "    \"p6138_i0\" : \"qualif_raw\",\n",
    "    \"p845_i0\" : \"age_education_raw\",\n",
    "    \"p20116_i0\" :\"smoking_raw\",\n",
    "    \"p1558_i0\" :\"alcohol_raw\", \n",
    "    \"p21001_i0\" : \"BMI_raw\", \n",
    "    \"p191\": \"date_lost_followup\",\n",
    "    \"p6150_i0\": \"self_report_cvd_baseline\", \n",
    "    \"p6150_i1\": \"self_report_cvd_inst_1\", \n",
    "    \"p6150_i2\": \"self_report_cvd_inst_2\", \n",
    "    \"p53_i0\": \"date_baseline\", \n",
    "    \"p53_i1\": \"date_inst_1\", \n",
    "    \"p53_i2\": \"date_inst_2\", \n",
    "    \"p90016\" : \"quality_good_calibration\",\n",
    "    \"p90183\" : \"clips_before_cal\",\n",
    "    \"p90185\" : \"clips_after_cal\", \n",
    "    \"p90187\" : \"total_reads\",\n",
    "    \"p90015\" :\"quality_good_wear_time\",\n",
    "    \"p90012\": \"overall_activity\", \n",
    "    \"p90011\": \"date_end_accel\",\n",
    "    \"p20002\": \"conditions_at_baseline_2006\"\n",
    "    }\n",
    "\n",
    "# We'll then auto-add names for other fields \n",
    "dataset_fields = [participant.find_field(name=x) for x in field_list] # get the full field info, not just the name\n",
    "for field in dataset_fields:\n",
    "    if field.name not in field_list_aliases.keys():\n",
    "        field_list_aliases[field.name] = field.title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adee6b8",
   "metadata": {},
   "source": [
    "We retrieve relevant data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b230bc7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-05 14:09:18.520 WARN  ShellBasedUnixGroupsMapping:210 - unable to return groups for user VZ33590Qf7vk0vzPk8ZfJjJYXpV5820y5k7KkKk7__project-GXJBY38JZ32Vb0588YVYx3Gy\n",
      "PartialGroupNameException The user name 'VZ33590Qf7vk0vzPk8ZfJjJYXpV5820y5k7KkKk7__project-GXJBY38JZ32Vb0588YVYx3Gy' is not found. id: ‘VZ33590Qf7vk0vzPk8ZfJjJYXpV5820y5k7KkKk7__project-GXJBY38JZ32Vb0588YVYx3Gy’: no such user\n",
      "id: ‘VZ33590Qf7vk0vzPk8ZfJjJYXpV5820y5k7KkKk7__project-GXJBY38JZ32Vb0588YVYx3Gy’: no such user\n",
      "\n",
      "\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolvePartialGroupNames(ShellBasedUnixGroupsMapping.java:294)\n",
      "\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:207)\n",
      "\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:97)\n",
      "\tat org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:51)\n",
      "\tat org.apache.hadoop.security.Groups$GroupCacheLoader.fetchGroupList(Groups.java:387)\n",
      "\tat org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:321)\n",
      "\tat org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:270)\n",
      "\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)\n",
      "\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)\n",
      "\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)\n",
      "\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)\n",
      "\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache.get(LocalCache.java:3962)\n",
      "\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3985)\n",
      "\tat org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4946)\n",
      "\tat org.apache.hadoop.security.Groups.getGroups(Groups.java:228)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.getGroups(UserGroupInformation.java:1734)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1722)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:517)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:254)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3650)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMetaStoreClient(Hive.java:3696)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.lambda$getMSC$0(Hive.java:3770)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3768)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3682)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1600)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1588)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:396)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:305)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:236)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:235)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:285)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:396)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)\n",
      "\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:170)\n",
      "\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:168)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$2(HiveSessionStateBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:119)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:119)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupGlobalTempView(SessionCatalog.scala:940)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveTempViews$$lookupTempView(Analyzer.scala:950)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveTempViews$$lookupAndResolveTempView(Analyzer.scala:963)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$$anonfun$apply$12.applyOrElse(Analyzer.scala:901)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$$anonfun$apply$12.applyOrElse(Analyzer.scala:899)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1122)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1121)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:206)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1148)\n",
      "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1147)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:390)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1122)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1121)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:206)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$.apply(Analyzer.scala:899)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1164)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1131)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:222)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:218)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:167)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:218)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:203)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:202)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:183)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:183)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:75)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:73)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:65)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "participant_data = participant.retrieve_fields(names=field_list, engine=dxdata.connect(), coding_values=\"replace\", column_aliases=field_list_aliases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b991fae4",
   "metadata": {},
   "source": [
    "Setting coding values to \"replace\" in this function means we get variables' values (rather than the coded format they are stored in)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dae038",
   "metadata": {},
   "source": [
    "As we will analyse only those participants with accelerometer data, we drop participants without accelerometer data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dcc04b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "participant_wacc_data = participant_data.filter(participant_data.date_end_accel.isNotNull()) # the end time of accelerometer wear should be present for all\n",
    "# participants with accelerometer data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0990eb29",
   "metadata": {},
   "source": [
    "We now write the data out as a csv file. \n",
    "\n",
    "Variables for which the value is itself an array (the usual format where the participant could tick several boxes) cannot be written to csv. We convert the affected variables to string variables: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf454af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "arrayed_cols = [\"qualif_raw\", \"self_report_cvd_baseline\", \"self_report_cvd_inst_1\", \"self_report_cvd_inst_2\"]\n",
    "for col in arrayed_cols:\n",
    "    participant_wacc_data = participant_wacc_data.withColumn(col, concat_ws(\"|\", col))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941db893",
   "metadata": {},
   "source": [
    "Spark stores and works with the data in several parts, so we coalesce them into one part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcfb9f3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "participant_wacc_data = participant_wacc_data.coalesce(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af72c96",
   "metadata": {},
   "source": [
    "We then set options and write to a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fd30405",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-05 14:09:50.040 WARN  package:69 - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "CSV data source does not support array<string> data type.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mparticipant_wacc_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparticipant_wacc_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/spark/python/pyspark/sql/readwriter.py:955\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression, sep\u001b[38;5;241m=\u001b[39msep, quote\u001b[38;5;241m=\u001b[39mquote, escape\u001b[38;5;241m=\u001b[39mescape, header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m    949\u001b[0m                nullValue\u001b[38;5;241m=\u001b[39mnullValue, escapeQuotes\u001b[38;5;241m=\u001b[39mescapeQuotes, quoteAll\u001b[38;5;241m=\u001b[39mquoteAll,\n\u001b[1;32m    950\u001b[0m                dateFormat\u001b[38;5;241m=\u001b[39mdateFormat, timestampFormat\u001b[38;5;241m=\u001b[39mtimestampFormat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    953\u001b[0m                charToEscapeQuoteEscaping\u001b[38;5;241m=\u001b[39mcharToEscapeQuoteEscaping,\n\u001b[1;32m    954\u001b[0m                encoding\u001b[38;5;241m=\u001b[39mencoding, emptyValue\u001b[38;5;241m=\u001b[39memptyValue, lineSep\u001b[38;5;241m=\u001b[39mlineSep)\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/cluster/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: CSV data source does not support array<string> data type."
     ]
    }
   ],
   "source": [
    "participant_wacc_data.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"participant_wacc_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932ebb75",
   "metadata": {},
   "source": [
    "Currently it is saved only in this session, so we need to upload to storage for reuse. As it was written by Spark, it is also stored in the \"hdfs\" (Hadoop Distributed File System), in a structured folder, so we first get it out of that and into \"standard\" storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dbcaf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -copyToLocal /user/root/participant_wacc_data participant_wacc_data # this gets it out of the hdfs\n",
    "dx upload participant_wacc_data/*.csv --dest /users/mcgaghd/Data/participant_wacc_data.csv # this uploads to the permanent storage on the RAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e96a5c",
   "metadata": {},
   "source": [
    "## Hospital data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d92c5",
   "metadata": {},
   "source": [
    "There are several sources of data on health and disease outcomes in the UK Biobank dataset. These include data from inpatient hospital admissions (sometimes referred to as \"Hospital Episode Statistics\"), death registry data, cancer registry data, primary care data on some participants/for some purposes, and some outcomes added to the tabular data (including the \"algorithmically defined outcomes\" identifying particular diseases across different data sources). \n",
    "\n",
    "For this analysis (following [1](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1003487)), we will use hospital inpatient data on diagnoses and death data.\n",
    "\n",
    "We will use two tables from the hospital inpatient data: hesin and hesin_diag. An entry in hesin is associated with a single hospital *episode* (generally: [\"a continuous period\n",
    "of admitted patient care administered under one consultant within that one hospital provider\"](https://biobank.ndph.ox.ac.uk/ukb/ukb/docs/HospitalEpisodeStatistics.pdf)), and provides information such as dates associated with the episode. Entries in hesin_diag are associated with a particular diagnosis within a particular episode.\n",
    "\n",
    "In this section, we will access, process, and save it.\n",
    "\n",
    "We load the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e084c5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hesin = dataset[\"hesin\"]\n",
    "hesin_diag = dataset[\"hesin_diag\"]\n",
    "\n",
    "hesin_data = hesin.retrieve_fields(engine=dxdata.connect())\n",
    "hesin_diag_data = hesin_diag.retrieve_fields(engine=dxdata.connect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1208c3e",
   "metadata": {},
   "source": [
    "A small proportion of episodes are missing an episode start date but have different dates associated with the episode. In an optional additional step, we make a new \"dateepiimp\" column which takes the value of \"epistart\", but when that is missing imputes them with \"disdate\" [NOTE - 2022-10-24, UK Biobank's own protocol when producing derived fields is to impute with admidate, epiend and disdate (see https://biobank.ndph.ox.ac.uk/showcase/ukb/docs/first_occurrences_outcomes.pdf, page 8). This notebook will be updated to match.] :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2af5fd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hesin_data = hesin_data.withColumn(\"dateepiimp\",\n",
    "                                   when(hesin_data[\"epistart\"].isNotNull(), hesin_data[\"epistart\"]).otherwise(hesin_data[\"disdate\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3655691",
   "metadata": {},
   "source": [
    " To associate dates with diagnoses, we need to join hesin and hesin_diag. We join on \"eid\" (participant ID), \"ins_index\" (the hospital episode in the data) and \"dnx_hesin_id\" (which is a combination of the participant ID and the instance index and so can be used to join uniquely - we join on all three just to avoid duplicating columns). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c57cd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hes_data = hesin_data.join(\n",
    "    hesin_diag_data, \n",
    "    [\"eid\", \"ins_index\", \"dnx_hesin_id\"],\n",
    "    \"left_outer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b42bc8",
   "metadata": {},
   "source": [
    "We keep only those participants with accelerometry data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3475aed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wacc_data =  participant_wacc_data.select(\"eid\").rdd.flatMap(lambda x: x).collect()\n",
    "hes_wacc_data = hes_data.filter(hes_data.eid.isin(wacc_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9351e21a",
   "metadata": {},
   "source": [
    "As for the tabular participant data, we coalesce to a single part and write out to a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c011c0ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hes_wacc_data.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"hes_wacc_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ff31bc",
   "metadata": {},
   "source": [
    "And again upload it to storage so we can reuse it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbd3a48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -copyToLocal /user/root/hes_wacc_data hes_wacc_data\n",
    "dx upload hes_wacc_data/*.csv --dest /users/mcgaghd/Data/hes_wacc_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a19e50d",
   "metadata": {},
   "source": [
    "# Death data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521885f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "death = dataset[\"death\"]\n",
    "death_cause = dataset[\"death_cause\"]\n",
    "\n",
    "death_data = death.retrieve_fields(engine=dxdata.connect())\n",
    "death_cause_data = death_cause.retrieve_fields(engine=dxdata.connect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618646d0",
   "metadata": {},
   "source": [
    "To reduce the size of the dataset, we keep only those participants with accelerometry data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d67cad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "death_wacc_data = death_data.filter(death_data.eid.isin(wacc_data))\n",
    "death_cause_wacc_data = death_cause_data.filter(death_cause_data.eid.isin(wacc_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629bc1d4",
   "metadata": {},
   "source": [
    "We now write it out to a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fb7a90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "death_wacc_data.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"death_wacc_data\")\n",
    "death_cause_wacc_data.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"death_cause_wacc_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d51ffa5",
   "metadata": {},
   "source": [
    "And again upload it to storage so we can reuse it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdbad3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -copyToLocal /user/root/death_wacc_data death_wacc_data\n",
    "hdfs dfs -copyToLocal /user/root/death_cause_wacc_data death_cause_wacc_data\n",
    "dx upload death_wacc_data/*.csv --dest /users/mcgaghd/Data/death_wacc_data.csv\n",
    "dx upload death_cause_wacc_data/*.csv --dest /users/mcgaghd/Data/death_cause_wacc_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0dc494-376f-493a-875d-b41c8e22446f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af54b4e2-8049-4a48-9e0a-3d75030acfaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18840da9-bf6b-4e98-abec-37a3b5dec767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "fbf01a0adb9bb315a48b04ad23dc43d230ab447260c5925410de5a91a9cd28c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
